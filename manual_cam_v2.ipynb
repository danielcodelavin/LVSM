{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b40f20",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from easydict import EasyDict as edict\n",
    "from PIL import Image\n",
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import display, clear_output\n",
    "# Add the root directory to the path for imports\n",
    "if os.path.dirname(os.getcwd()) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(os.getcwd()))\n",
    "# ======== CONFIGURATION ========\n",
    "# Set your paths and configuration here\n",
    "CONFIG = {\n",
    "    # Dataset path\n",
    "    \"dataset_path\": \"/home/stud/lavingal/storage/slurm/lavingal/LVSM/datasets/re10k/test/full_list.txt\",\n",
    "    \n",
    "    # Specific checkpoint to load (set to empty to use latest from checkpoint_dir)\n",
    "    \"specific_checkpoint\": \"/home/stud/lavingal/storage/slurm/lavingal/experiments/checkpoints/LVSM_scene_decoder_only/ckpt_0000000000246000.pt\",  # e.g. \"/path/to/specific/ckpt_0000000000123456.pt\"\n",
    "    \n",
    "    # Checkpoint directory (used if specific_checkpoint is empty)\n",
    "     \"checkpoint_dir\": \"../experiments/checkpoints/LVSM_scene_decoder_only\",\n",
    "    \n",
    "    # Device to use\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \n",
    "    # Model configuration file (will be overridden with loaded values)\n",
    "    \"config_file\": \"/home/stud/lavingal/storage/slurm/lavingal/LVSM/configs/LVSM_scene_decoder_only.yaml\",\n",
    "\n",
    "    # Camera modification ranges\n",
    "    \"tx_range\": (-6.0, 6.0),  # Translation X range\n",
    "    \"ty_range\": (-6.0, 6.0),  # Translation Y range\n",
    "    \"tz_range\": (-6.0, 6.0),  # Translation Z range\n",
    "    \"rx_range\": (-90.0, 90.0),  # Rotation X range (degrees)\n",
    "    \"ry_range\": (-90.0, 90.0),  # Rotation Y range (degrees)\n",
    "    \"rz_range\": (-90.0, 90.0),  # Rotation Z range (degrees)\n",
    "}\n",
    "# ======== HELPER FUNCTIONS ========\n",
    "def load_config(config_path=None):\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    if config_path and os.path.exists(config_path):\n",
    "        import yaml\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        # Use default config from the provided YAML content\n",
    "        config = {\n",
    "            \"model\": {\n",
    "                \"class_name\": \"model.LVSM_scene_decoder_only.Images2LatentScene\",\n",
    "                \"image_tokenizer\": {\n",
    "                    \"image_size\": 256,\n",
    "                    \"patch_size\": 8,\n",
    "                    \"in_channels\": 9  # 3 RGB + 3 direction + 3 Reference\n",
    "                },\n",
    "                \"target_pose_tokenizer\": {\n",
    "                    \"image_size\": 256,\n",
    "                    \"patch_size\": 8,\n",
    "                    \"in_channels\": 6  # 3 direction + 3 Reference\n",
    "                },\n",
    "                \"transformer\": {\n",
    "                    \"d\": 768,\n",
    "                    \"d_head\": 64,\n",
    "                    \"n_layer\": 6,\n",
    "                    \"special_init\": True,\n",
    "                    \"depth_init\": True,\n",
    "                    \"use_qk_norm\": True\n",
    "                }\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"amp_dtype\": \"bf16\",\n",
    "                \"batch_size_per_gpu\": 1,\n",
    "                \"center_crop\": True,\n",
    "                \"scene_scale_factor\": 1.35,\n",
    "                \"checkpoint_dir\": CONFIG[\"checkpoint_dir\"],\n",
    "                \"dataset_name\": \"data.dataset_scene.Dataset\",\n",
    "                \"dataset_path\": CONFIG[\"dataset_path\"],\n",
    "                \"num_input_views\": 2,\n",
    "                \"num_target_views\": 6,\n",
    "                \"num_threads\": 8,\n",
    "                \"num_views\": 8,\n",
    "                \"num_workers\": 1,\n",
    "                \"square_crop\": True,\n",
    "                \"target_has_input\": True,\n",
    "                \"use_amp\": False,\n",
    "                \"use_tf32\": False,\n",
    "                \"view_selector\": {\n",
    "                    \"max_frame_dist\": 192,\n",
    "                    \"min_frame_dist\": 25\n",
    "                }\n",
    "            },\n",
    "            \"inference\": {\n",
    "                \"render_video\": False,\n",
    "                \"render_video_config\": {\n",
    "                    \"traj_type\": \"interpolate\",\n",
    "                    \"num_frames\": 10,\n",
    "                    \"loop_video\": False,\n",
    "                    \"order_poses\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Convert to EasyDict for easier access\n",
    "    config = edict(config)\n",
    "    \n",
    "    # Override with our specific settings\n",
    "    config.training.dataset_path = CONFIG[\"dataset_path\"]\n",
    "    config.training.checkpoint_dir = CONFIG[\"checkpoint_dir\"]\n",
    "    config.training.batch_size_per_gpu = 1\n",
    "    config.training.num_workers = 1\n",
    "    config.training.use_amp = False\n",
    "    \n",
    "    return config\n",
    "def load_model(config):\n",
    "    \"\"\"Load the LVSM model and checkpoint\"\"\"\n",
    "    # Set up mock distributed environment to avoid DDP-related errors\n",
    "    import torch.distributed as dist\n",
    "    if not dist.is_available() or not dist.is_initialized():\n",
    "        import random\n",
    "        # Use random port to avoid conflicts\n",
    "        random_port = random.randint(29500, 65000)\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = str(random_port)\n",
    "        os.environ['RANK'] = '0'\n",
    "        os.environ['WORLD_SIZE'] = '1'\n",
    "        if dist.is_available():\n",
    "            try:\n",
    "                dist.init_process_group(backend='gloo', rank=0, world_size=1)\n",
    "            except RuntimeError:\n",
    "                print(\"Warning: Could not initialize distributed environment. Continuing without it.\")\n",
    "    \n",
    "    # Initialize LPIPS separately to avoid distributed initialization errors\n",
    "    import lpips\n",
    "    # Suppress future warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    # Create a custom class that inherits from the original but overrides problematic methods\n",
    "    module, class_name = config.model.class_name.rsplit(\".\", 1)\n",
    "    LVSM_Base = importlib.import_module(module).__dict__[class_name]\n",
    "    \n",
    "    class LVSMSingleDevice(LVSM_Base):\n",
    "        def __init__(self, config):\n",
    "            # Save original config settings\n",
    "            orig_l2_weight = config.training.l2_loss_weight\n",
    "            orig_lpips_weight = config.training.lpips_loss_weight\n",
    "            orig_perceptual_weight = config.training.perceptual_loss_weight\n",
    "            \n",
    "            # Temporarily disable loss components to avoid DDP initialization\n",
    "            config.training.l2_loss_weight = 1.0\n",
    "            config.training.lpips_loss_weight = 0.0\n",
    "            config.training.perceptual_loss_weight = 0.0\n",
    "            \n",
    "            # Initialize the base class\n",
    "            super().__init__(config)\n",
    "            \n",
    "            # Restore original config settings\n",
    "            config.training.l2_loss_weight = orig_l2_weight\n",
    "            config.training.lpips_loss_weight = orig_lpips_weight\n",
    "            config.training.perceptual_loss_weight = orig_perceptual_weight\n",
    "    \n",
    "    # Create model instance\n",
    "    model = LVSMSingleDevice(config).to(CONFIG[\"device\"])\n",
    "    \n",
    "    # Load checkpoint\n",
    "    ckpt_path = CONFIG[\"specific_checkpoint\"] if CONFIG[\"specific_checkpoint\"] else config.training.checkpoint_dir\n",
    "    model.load_ckpt(ckpt_path)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "def load_dataset(config):\n",
    "    \"\"\"Load the dataset\"\"\"\n",
    "    dataset_name = config.training.dataset_name\n",
    "    module, class_name = dataset_name.rsplit(\".\", 1)\n",
    "    Dataset = importlib.import_module(module).__dict__[class_name]\n",
    "    dataset = Dataset(config)\n",
    "    return dataset\n",
    "def show_image_grid(images, titles=None, figsize=(15, 15), rows=None, cols=None):\n",
    "    \"\"\"Display a grid of images\"\"\"\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        # Convert from bfloat16 to float32 if needed\n",
    "        if images.dtype == torch.bfloat16:\n",
    "            images = images.to(torch.float32)\n",
    "        # Convert from tensor [B, C, H, W] to numpy [B, H, W, C]\n",
    "        images = images.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    # Determine grid size\n",
    "    n_images = len(images)\n",
    "    if rows is None and cols is None:\n",
    "        cols = int(np.ceil(np.sqrt(n_images)))\n",
    "        rows = int(np.ceil(n_images / cols))\n",
    "    elif rows is None:\n",
    "        rows = int(np.ceil(n_images / cols))\n",
    "    elif cols is None:\n",
    "        cols = int(np.ceil(n_images / rows))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows * cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot images\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n_images:\n",
    "            if images[i].shape[-1] == 1:  # Grayscale\n",
    "                ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "            else:\n",
    "                # Ensure values are in proper range for display\n",
    "                img = np.clip(images[i], 0, 1)\n",
    "                ax.imshow(img)\n",
    "            \n",
    "            if titles is not None and i < len(titles):\n",
    "                ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def process_c2w_matrix(c2w):\n",
    "    \"\"\"Process camera-to-world matrix for display\"\"\"\n",
    "    # Extract rotation and translation\n",
    "    rotation = c2w[:3, :3]\n",
    "    translation = c2w[:3, 3]\n",
    "    \n",
    "    # Convert rotation to Euler angles (in degrees)\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    euler = Rotation.from_matrix(rotation.cpu().numpy()).as_euler('xyz', degrees=True)\n",
    "    \n",
    "    return {\n",
    "        'translation': translation.cpu().numpy(),\n",
    "        'rotation_euler_deg': euler,\n",
    "        'full_matrix': c2w.cpu().numpy()\n",
    "    }\n",
    "def get_ray_bundle(fxfycxcy, c2w, height, width, device=\"cpu\"):\n",
    "    \"\"\"Generate rays for a camera view\"\"\"\n",
    "    # Create pixel coordinates\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(width, device=device),\n",
    "        torch.arange(height, device=device),\n",
    "        indexing=\"ij\"\n",
    "    )\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    \n",
    "    # Convert to normalized device coordinates\n",
    "    fx, fy, cx, cy = fxfycxcy\n",
    "    dirs = torch.stack(\n",
    "        [\n",
    "            (i - cx) / fx,\n",
    "            -(j - cy) / fy,\n",
    "            -torch.ones_like(i),\n",
    "        ],\n",
    "        dim=-1,\n",
    "    )\n",
    "    \n",
    "    # Transform ray directions to world space\n",
    "    rays_d = dirs @ c2w[:3, :3].t()\n",
    "    rays_d = rays_d / torch.linalg.norm(rays_d, dim=-1, keepdim=True)\n",
    "    \n",
    "    # Get ray origins from camera position\n",
    "    rays_o = c2w[:3, 3].expand(rays_d.shape)\n",
    "    \n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c6413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01be0623",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_camera_in_world(position, rotation_euler_deg, device=None):\n",
    "    \"\"\"\n",
    "    Create a camera-to-world matrix directly from position and rotation in world coordinates\n",
    "    \n",
    "    Args:\n",
    "        position: [x, y, z] camera position in world coordinates\n",
    "        rotation_euler_deg: [rx, ry, rz] rotation in degrees (Euler angles, XYZ order)\n",
    "        device: Device to place tensor on (default: CONFIG[\"device\"])\n",
    "        \n",
    "    Returns:\n",
    "        c2w: 4x4 camera-to-world transformation matrix\n",
    "    \"\"\"\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    if device is None:\n",
    "        device = CONFIG[\"device\"]\n",
    "    \n",
    "    # Create rotation matrix from Euler angles\n",
    "    r = Rotation.from_euler('xyz', rotation_euler_deg, degrees=True)\n",
    "    rotation_matrix = r.as_matrix()\n",
    "    \n",
    "    # Create camera-to-world matrix\n",
    "    c2w = torch.eye(4)\n",
    "    c2w[:3, :3] = torch.tensor(rotation_matrix, dtype=torch.float32)\n",
    "    c2w[:3, 3] = torch.tensor(position, dtype=torch.float32)\n",
    "    \n",
    "    return c2w.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "513f3580",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 7286 scenes\n",
      "Model loaded from /home/stud/lavingal/storage/slurm/lavingal/experiments/checkpoints/LVSM_scene_decoder_only/ckpt_0000000000246000.pt\n"
     ]
    }
   ],
   "source": [
    "# ======== MAIN CODE ========\n",
    "# Load configuration\n",
    "config = load_config(CONFIG.get(\"config_file\"))\n",
    "# Modify config to avoid distributed training issues\n",
    "config.training.use_amp = False\n",
    "config.training.use_tf32 = False\n",
    "# Load dataset first\n",
    "dataset = load_dataset(config)\n",
    "print(f\"Dataset loaded with {len(dataset)} scenes\")\n",
    "# Now load model with modified configuration\n",
    "try:\n",
    "    model = load_model(config)\n",
    "    print(f\"Model loaded from {CONFIG['specific_checkpoint'] if CONFIG['specific_checkpoint'] else config.training.checkpoint_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "# ======== SCENE SELECTION ========\n",
    "def select_scene(scene_idx=0):\n",
    "    \"\"\"Select a scene and display input and target views\"\"\"\n",
    "    if scene_idx < 0 or scene_idx >= len(dataset):\n",
    "        print(f\"Scene index out of range. Please choose between 0 and {len(dataset)-1}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get scene data\n",
    "        scene_data = dataset[scene_idx]\n",
    "        \n",
    "        # Move to device\n",
    "        scene_data = {k: v.to(CONFIG[\"device\"]) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in scene_data.items()}\n",
    "        \n",
    "        # Get batch dimension right\n",
    "        for k, v in scene_data.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dim() > 0:\n",
    "                scene_data[k] = v.unsqueeze(0)\n",
    "        \n",
    "        # Process data through model's process_data function\n",
    "        with torch.no_grad():\n",
    "            input_data, target_data = model.process_data(\n",
    "                scene_data, \n",
    "                has_target_image=True, \n",
    "                target_has_input=config.training.target_has_input, \n",
    "                compute_rays=True\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing scene {scene_idx}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Display input views\n",
    "    print(\"\\n=== Input Views ===\")\n",
    "    input_images = input_data.image.squeeze(0)  # [v, c, h, w]\n",
    "    show_image_grid(input_images, titles=[f\"Input View {i}\" for i in range(input_images.shape[0])])\n",
    "    \n",
    "    # Display camera parameters for input views\n",
    "    print(\"\\n=== Input Camera Parameters ===\")\n",
    "    for i in range(input_data.c2w.shape[1]):\n",
    "        c2w_info = process_c2w_matrix(input_data.c2w[0, i])\n",
    "        print(f\"Input View {i}:\")\n",
    "        print(f\"  Translation: {c2w_info['translation']}\")\n",
    "        print(f\"  Rotation (Euler XYZ, degrees): {c2w_info['rotation_euler_deg']}\")\n",
    "        print()\n",
    "    \n",
    "    # Display target views\n",
    "    print(\"\\n=== Target Views ===\")\n",
    "    target_images = target_data.image.squeeze(0)  # [v, c, h, w]\n",
    "    show_image_grid(target_images, titles=[f\"Target View {i}\" for i in range(target_images.shape[0])])\n",
    "    \n",
    "    # Display camera parameters for target views\n",
    "    print(\"\\n=== Target Camera Parameters ===\")\n",
    "    for i in range(target_data.c2w.shape[1]):\n",
    "        c2w_info = process_c2w_matrix(target_data.c2w[0, i])\n",
    "        print(f\"Target View {i}:\")\n",
    "        print(f\"  Translation: {c2w_info['translation']}\")\n",
    "        print(f\"  Rotation (Euler XYZ, degrees): {c2w_info['rotation_euler_deg']}\")\n",
    "        print()\n",
    "    \n",
    "    return {\"input\": input_data, \"target\": target_data, \"scene_data\": scene_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac188c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ======== CUSTOM CAMERA PARAMETERS (RELATIVE METHOD) ========\n",
    "def create_custom_camera(base_camera, tx=0.0, ty=0.0, tz=0.0, rx=0.0, ry=0.0, rz=0.0):\n",
    "    \"\"\"Create a custom camera by modifying the base camera parameters\"\"\"\n",
    "    # Start with the base camera\n",
    "    c2w = base_camera.clone()\n",
    "    \n",
    "    # Create rotation matrices for the adjustments\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    r = Rotation.from_euler('xyz', [rx, ry, rz], degrees=True)\n",
    "    rot_matrix = torch.tensor(r.as_matrix(), dtype=c2w.dtype, device=c2w.device)\n",
    "    \n",
    "    # Apply rotation to the existing rotation matrix\n",
    "    c2w[:3, :3] = torch.matmul(rot_matrix, c2w[:3, :3])\n",
    "    \n",
    "    # Apply translation adjustments\n",
    "    c2w[0, 3] += tx\n",
    "    c2w[1, 3] += ty\n",
    "    c2w[2, 3] += tz\n",
    "    \n",
    "    return c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21de73bc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def render_with_custom_camera(scene_data, input_data, target_data, base_camera_idx=0, tx=0.0, ty=0.0, tz=0.0, rx=0.0, ry=0.0, rz=0.0):\n",
    "    \"\"\"Render the scene with a custom camera (relative to base camera)\"\"\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Create custom camera\n",
    "            base_c2w = input_data.c2w[0, base_camera_idx].clone()\n",
    "            custom_c2w = create_custom_camera(base_c2w, tx, ty, tz, rx, ry, rz)\n",
    "            \n",
    "            # Create a copy of the original scene data\n",
    "            raw_scene_data = {k: v.clone() if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in scene_data[\"scene_data\"].items()}\n",
    "            \n",
    "            # Modify ALL target views to use our custom camera\n",
    "            num_input_views = config.training.num_input_views\n",
    "            num_target_views = config.training.num_target_views\n",
    "            \n",
    "            for i in range(num_target_views):\n",
    "                target_view_idx = num_input_views + i\n",
    "                raw_scene_data[\"c2w\"][0, target_view_idx] = custom_c2w\n",
    "            \n",
    "            # Use autocast like in your inference.py\n",
    "            with torch.cuda.amp.autocast(\n",
    "                enabled=True,\n",
    "                dtype=torch.bfloat16\n",
    "            ):\n",
    "                # Turn off gradient checkpointing by modifying config temporarily\n",
    "                orig_checkpoint_every = config.training.grad_checkpoint_every\n",
    "                config.training.grad_checkpoint_every = 999999\n",
    "                \n",
    "                # Process through model\n",
    "                result = model(raw_scene_data, has_target_image=False)\n",
    "                \n",
    "                # Restore config\n",
    "                config.training.grad_checkpoint_every = orig_checkpoint_every\n",
    "            \n",
    "            # Print shape info for debugging\n",
    "            print(f\"Result.render shape: {result.render.shape}\")\n",
    "            \n",
    "            # Get rendered image\n",
    "            rendered_image = result.render[:, 0].to(torch.float32)\n",
    "            \n",
    "            # Display camera parameters\n",
    "            c2w_info = process_c2w_matrix(custom_c2w)\n",
    "            print(\"\\n=== Custom Camera Parameters ===\")\n",
    "            print(f\"  Translation: {c2w_info['translation']}\")\n",
    "            print(f\"  Rotation (Euler XYZ, degrees): {c2w_info['rotation_euler_deg']}\")\n",
    "            print(f\"  Relative to input view: {base_camera_idx}\")\n",
    "            \n",
    "            # Display the rendered image\n",
    "            print(\"\\n=== Custom Camera Rendering ===\")\n",
    "            show_image_grid(rendered_image, titles=[\"Rendered View\"])\n",
    "            \n",
    "            return rendered_image\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering with custom camera: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3eb7744",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ======== WORLD COORDINATE CAMERA CONTROL ========\n",
    "def render_with_world_camera(scene_data, position, rotation_euler_deg):\n",
    "    \"\"\"\n",
    "    Render the scene with a camera defined directly in world coordinates\n",
    "    \n",
    "    Args:\n",
    "        scene_data: The scene data dictionary\n",
    "        position: [x, y, z] camera position in world coordinates\n",
    "        rotation_euler_deg: [rx, ry, rz] rotation in degrees (Euler angles, XYZ order)\n",
    "        \n",
    "    Returns:\n",
    "        rendered_image: The rendered image tensor\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # Create camera directly in world coordinates\n",
    "            custom_c2w = create_camera_in_world(position, rotation_euler_deg)\n",
    "            \n",
    "            # Create a copy of the original scene data\n",
    "            raw_scene_data = {k: v.clone() if isinstance(v, torch.Tensor) else v \n",
    "                            for k, v in scene_data[\"scene_data\"].items()}\n",
    "            \n",
    "            # We'll modify all target views to our new camera\n",
    "            # This ensures the model isn't using other target views with different cameras\n",
    "            num_input_views = config.training.num_input_views\n",
    "            num_target_views = config.training.num_target_views\n",
    "            \n",
    "            # Set all target views to use our custom camera\n",
    "            for i in range(num_target_views):\n",
    "                target_view_idx = num_input_views + i\n",
    "                raw_scene_data[\"c2w\"][0, target_view_idx] = custom_c2w\n",
    "            \n",
    "            # Use autocast like in your inference.py\n",
    "            with torch.cuda.amp.autocast(\n",
    "                enabled=True,\n",
    "                dtype=torch.bfloat16\n",
    "            ):\n",
    "                # Turn off gradient checkpointing by modifying config temporarily\n",
    "                orig_checkpoint_every = config.training.grad_checkpoint_every\n",
    "                config.training.grad_checkpoint_every = 999999\n",
    "                \n",
    "                # Process through model\n",
    "                result = model(raw_scene_data, has_target_image=False)\n",
    "                \n",
    "                # Restore config\n",
    "                config.training.grad_checkpoint_every = orig_checkpoint_every\n",
    "            \n",
    "            # Print shape info for debugging\n",
    "            print(f\"Result.render shape: {result.render.shape}\")\n",
    "            \n",
    "            # Get rendered image - just the first target view\n",
    "            rendered_image = result.render[:, 0].to(torch.float32)\n",
    "            \n",
    "            # Display camera parameters\n",
    "            c2w_info = process_c2w_matrix(custom_c2w)\n",
    "            print(\"\\n=== World Camera Parameters ===\")\n",
    "            print(f\"  Position: {position}\")\n",
    "            print(f\"  Rotation (Euler XYZ, degrees): {rotation_euler_deg}\")\n",
    "            \n",
    "            # Display the rendered image\n",
    "            print(\"\\n=== World Camera Rendering ===\")\n",
    "            show_image_grid(rendered_image, titles=[\"Rendered View\"])\n",
    "            \n",
    "            return rendered_image\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering with world camera: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40637c4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ======== INTERACTIVE WIDGET ========\n",
    "def run_interactive_demo():\n",
    "    \"\"\"Run an interactive demo to select scenes and customize camera parameters\"\"\"\n",
    "    scene_output = None\n",
    "    \n",
    "    def on_scene_select(scene_idx):\n",
    "        nonlocal scene_output\n",
    "        clear_output(wait=True)\n",
    "        scene_output = select_scene(scene_idx)\n",
    "    \n",
    "    # Create scene selection widget\n",
    "    scene_select = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(dataset)-1,\n",
    "        step=1,\n",
    "        description='Scene Index:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    # Connect the callback\n",
    "    scene_select.observe(lambda change: on_scene_select(change['new']), names='value')\n",
    "    \n",
    "    # Display the widget\n",
    "    display(scene_select)\n",
    "    \n",
    "    # Initialize with the first scene\n",
    "    on_scene_select(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fe9fb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ======== CUSTOM PARAMS CELLS ========\n",
    "def render_with_custom_params(scene_idx, base_camera_idx=0, tx=0.0, ty=0.0, tz=0.0, rx=0.0, ry=0.0, rz=0.0):\n",
    "    \"\"\"Render with hardcoded custom camera parameters (relative method)\"\"\"\n",
    "    # Get scene data\n",
    "    scene_output = select_scene(scene_idx)\n",
    "    if scene_output:\n",
    "        # Render with custom parameters\n",
    "        render_with_custom_camera(\n",
    "            scene_output,\n",
    "            scene_output[\"input\"],\n",
    "            scene_output[\"target\"],\n",
    "            base_camera_idx=base_camera_idx,\n",
    "            tx=tx, ty=ty, tz=tz,\n",
    "            rx=rx, ry=ry, rz=rz\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6051f715",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def render_with_world_params(scene_idx, position, rotation_euler_deg):\n",
    "    \"\"\"Render with hardcoded world coordinate camera parameters\"\"\"\n",
    "    # Get scene data\n",
    "    scene_output = select_scene(scene_idx)\n",
    "    if scene_output:\n",
    "        # Render with world coordinate parameters\n",
    "        render_with_world_camera(\n",
    "            scene_output,\n",
    "            position=position,\n",
    "            rotation_euler_deg=rotation_euler_deg\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7934d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== CAMERA PARAMETER SWEEP (RELATIVE METHOD) ========\n",
    "def run_parameter_sweep_relative(scene_idx=5, base_camera_idx=0, parameter=\"TY\", \n",
    "                                start_value=-0.2, end_value=0.2, increment=0.002, fps=6,\n",
    "                                output_dir=\"sweep_vids\"):\n",
    "    \"\"\"\n",
    "    Run a camera parameter sweep using the relative method\n",
    "    \n",
    "    Args:\n",
    "        scene_idx: Index of the scene to render\n",
    "        base_camera_idx: Which input camera to use as base\n",
    "        parameter: Parameter to sweep (\"TX\", \"TY\", \"TZ\", \"RX\", \"RY\", \"RZ\")\n",
    "        start_value: Starting value for the parameter\n",
    "        end_value: Ending value for the parameter\n",
    "        increment: Increment size for the parameter\n",
    "        fps: Frames per second for the output video\n",
    "        output_dir: Directory to save the output videos\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    import imageio\n",
    "    import time\n",
    "\n",
    "    # Make sure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Time the process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get scene data (only once)\n",
    "    scene_output = select_scene(scene_idx)\n",
    "\n",
    "    if scene_output:\n",
    "        # Prepare for image collection\n",
    "        frames = []\n",
    "        parameter_values = []\n",
    "        current_value = start_value\n",
    "        \n",
    "        # Show progress\n",
    "        total_frames = int((end_value - start_value) / increment) + 1\n",
    "        print(f\"Starting parameter sweep of {parameter} from {start_value} to {end_value} with increment {increment}\")\n",
    "        print(f\"Rendering approximately {total_frames} frames...\")\n",
    "        \n",
    "        # Parameter sweep loop\n",
    "        while current_value <= end_value:\n",
    "            # Set the camera parameters according to which parameter we're sweeping\n",
    "            tx, ty, tz, rx, ry, rz = 1.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            \n",
    "            if parameter == \"TX\":\n",
    "                tx = current_value\n",
    "            elif parameter == \"TY\":\n",
    "                ty = current_value\n",
    "            elif parameter == \"TZ\":\n",
    "                tz = current_value\n",
    "            elif parameter == \"RX\":\n",
    "                rx = current_value\n",
    "            elif parameter == \"RY\": \n",
    "                ry = current_value\n",
    "            elif parameter == \"RZ\":\n",
    "                rz = current_value\n",
    "            \n",
    "            # Render with the current parameter value\n",
    "            print(f\"Rendering {parameter}={current_value:.2f}...\")\n",
    "            rendered_img = render_with_custom_camera(\n",
    "                scene_output,\n",
    "                scene_output[\"input\"],\n",
    "                scene_output[\"target\"],\n",
    "                base_camera_idx=base_camera_idx,\n",
    "                tx=tx, ty=ty, tz=tz,\n",
    "                rx=rx, ry=ry, rz=rz\n",
    "            )\n",
    "            \n",
    "            # Convert tensor to numpy array and append to frames\n",
    "            if rendered_img is not None:\n",
    "                # Check the shape of the tensor to handle it correctly\n",
    "                print(f\"Rendered image shape: {rendered_img.shape}\")\n",
    "                \n",
    "                # The render_with_custom_camera function likely returns a tensor with shape [1, 3, H, W]\n",
    "                # We need to squeeze out any batch dimension if it exists\n",
    "                if len(rendered_img.shape) == 4:\n",
    "                    rendered_img = rendered_img.squeeze(0)  # Remove batch dimension if present\n",
    "                \n",
    "                # Now convert from tensor [C,H,W] to numpy [H,W,C]\n",
    "                img_np = rendered_img.permute(1, 2, 0).cpu().numpy()\n",
    "                # Clip values to valid range\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                frames.append((img_np * 255).astype(np.uint8))\n",
    "                parameter_values.append(current_value)\n",
    "            \n",
    "            # Increment the parameter\n",
    "            current_value += increment\n",
    "        \n",
    "        # Create the video file\n",
    "        if frames:\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            video_filename = os.path.join(output_dir, \n",
    "                                        f\"{parameter}_sweep_{start_value}_to_{end_value}_scene{scene_idx}.mp4\")\n",
    "            \n",
    "            print(f\"Creating video with {len(frames)} frames...\")\n",
    "            imageio.mimsave(video_filename, frames, fps=fps)\n",
    "            \n",
    "            # Also create a GIF for easy viewing\n",
    "            gif_filename = os.path.join(output_dir, \n",
    "                                    f\"{parameter}_sweep_{start_value}_to_{end_value}_scene{scene_idx}.gif\")\n",
    "            imageio.mimsave(gif_filename, frames, fps=fps, loop=0)\n",
    "            \n",
    "            print(f\"Video saved to: {video_filename}\")\n",
    "            print(f\"GIF saved to: {gif_filename}\")\n",
    "            \n",
    "            # Create a parameter value vs. frame number plot to help understand the relationship\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(range(len(parameter_values)), parameter_values)\n",
    "            plt.xlabel(\"Frame Number\")\n",
    "            plt.ylabel(f\"{parameter} Value\")\n",
    "            plt.title(f\"{parameter} Sweep Progression\")\n",
    "            plt.grid(True)\n",
    "            plot_filename = os.path.join(output_dir, \n",
    "                                        f\"{parameter}_sweep_plot_scene{scene_idx}.png\")\n",
    "            plt.savefig(plot_filename)\n",
    "            plt.close()\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Parameter sweep completed in {elapsed_time:.2f} seconds.\")\n",
    "            print(f\"Generated {len(frames)} frames.\")\n",
    "        else:\n",
    "             print(\"No frames were generated. Check for errors during rendering.\")\n",
    "    else:\n",
    "        print(f\"Failed to load scene {SCENE_INDEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c36f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (55553674.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31moutput_dir=\"v2_sweep_vids\"):\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "run_parameter_sweep_relative(scene_idx=5, base_camera_idx=0, parameter=\"TX\", \n",
    "                                start_value=-0.2, end_value=0.2, increment=0.002, fps=6,\n",
    "                                output_dir=\"v2_sweep_vids\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "LVSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
